{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FECp14-d_F2e"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za-DgcYB_IQx",
        "outputId": "059907a1-acc3-4683-8294-93c72c99150a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'assignment_1' already exists and is not an empty directory.\n",
            "'mv' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NLP-Reichman/assignment_1.git\n",
        "!mv assignment_1/data data\n",
        "!rm assignment_1/ -r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i2bOXTB8Dvc"
      },
      "source": [
        "# Introduction\n",
        "In this assignment you will be creating tools for learning and testing language models. The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n",
        "The relevant files are under the data folder:\n",
        "\n",
        "- en.csv (or the equivalent JSON file)\n",
        "- es.csv (or the equivalent JSON file)\n",
        "- fr.csv (or the equivalent JSON file)\n",
        "- in.csv (or the equivalent JSON file)\n",
        "- it.csv (or the equivalent JSON file)\n",
        "- nl.csv (or the equivalent JSON file)\n",
        "- pt.csv (or the equivalent JSON file)\n",
        "- tl.csv (or the equivalent JSON file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1u1qR7iaq_GU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import sys\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHN0tWTurwkN"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i56aKA0K8adr"
      },
      "source": [
        "## Part 1\n",
        "Implement the function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. Our token definition is a single UTF-8 encoded character. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data.\n",
        "\n",
        "Note - do NOT lowercase the sentences in whi HW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ws_5u7vRrg0o"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1806"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess() -> list[str]:\n",
        "  '''\n",
        "  Return a list of characters, representing the shared vocabulary of all languages\n",
        "  '''\n",
        "  df = pd.concat(map(pd.read_csv, glob.glob('assignment_1/data/*.csv')))\n",
        "  unique_chars = list(''.join(set(df['tweet_text'].str.cat(sep=''))))\n",
        "  unique_chars.append('<unk>')\n",
        "  unique_chars.append('<start>')\n",
        "  unique_chars.append('</end>')\n",
        "  return unique_chars\n",
        "\n",
        "unique_chars = preprocess()\n",
        "V = len(unique_chars)\n",
        "V\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpjtwHW08jyH"
      },
      "source": [
        "## Part 2\n",
        "Implement the function *lm* that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant *n*-1 sequences, and the values are dictionaries with the *n*_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{ \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25}, \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1} }\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uySEXdEUrkq_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "747"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def lm(lang: str, n: int) -> dict[str, dict[str, float]]:\n",
        "    '''\n",
        "    Return a language model for the given lang and n_gram (n)\n",
        "    :param lang: the language of the model\n",
        "    :param n: the n_gram value\n",
        "    :return: a dictionary where the keys are n_grams and the values are dictionaries\n",
        "    '''\n",
        "    df = pd.read_csv('assignment_1/data/' + lang + '.csv')\n",
        "    # print(df.head())\n",
        "    text = ''.join(df['tweet_text'].str.cat(sep=''))\n",
        "\n",
        "    lm = {}\n",
        "\n",
        "    for i in range(len(text)-n+1):\n",
        "        ngram = text[i:i+n-1]\n",
        "        suffix = text[i+n-1]\n",
        "\n",
        "        if ngram not in lm:\n",
        "            lm[ngram] = {}\n",
        "            lm[ngram][suffix] = 1\n",
        "\n",
        "        else:\n",
        "            if suffix in lm[ngram]:\n",
        "                lm[ngram][suffix] += 1\n",
        "            else:\n",
        "                lm[ngram][suffix] = 1\n",
        "\n",
        "    # print(lm)\n",
        "\n",
        "    for ngram in lm:\n",
        "        sum_ngram = sum(lm[ngram].values())\n",
        "        for suffix in lm[ngram]:\n",
        "            lm[ngram][suffix] = lm[ngram][suffix] / sum_ngram\n",
        "\n",
        "    # print(lm)\n",
        "\n",
        "    return lm\n",
        "\n",
        "\n",
        "ngrams = lm('en', 2)\n",
        "len(ngrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwZnk7Ke8rW5"
      },
      "source": [
        "## Part 3\n",
        "Implement the function *eval* that returns the perplexity of a model (dictionary) running over the data file of the given target language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ef-EglxXrmk2"
      },
      "outputs": [],
      "source": [
        "def eval(model: dict, target_lang: str) -> float:\n",
        "  '''\n",
        "  Return the perplexity value calculated over applying the model on the text file\n",
        "  of the target_lang language.\n",
        "  :param model: the language model\n",
        "  :param target_lang: the target language\n",
        "  :return: the perplexity value\n",
        "  '''\n",
        "  df = pd.read_csv('assignment_1/data/' + target_lang + '.csv')\n",
        "  # for row in df.iterrows():\n",
        "  #   print(row)\n",
        "  sum_perplexity = 0\n",
        "  for index, row in df.iterrows():\n",
        "    # print(row['tweet_text'])\n",
        "    sum_perplexity += perplexity(row['tweet_text'], model)\n",
        "  \n",
        "  sum_perplexity /= (len(df) - 1)\n",
        "  return sum_perplexity\n",
        "\n",
        "def perplexity(sentence, model):\n",
        "  n = len(next(iter((model))))\n",
        "  length = len(sentence)\n",
        "  perp = 1.0\n",
        "\n",
        "  for i in range(len(sentence)-n):\n",
        "    ngram = sentence[i:i+n]\n",
        "    suffix = sentence[i+n]\n",
        "    # print(ngram, suffix)\n",
        "\n",
        "    if ngram in model:\n",
        "      if suffix in model[ngram]:\n",
        "        perp *= model[ngram][suffix]\n",
        "      else: perp *= 1/V\n",
        "    else: perp *= 1/V\n",
        "  # print(sentence)\n",
        "  if perp == 0: perp = sys.float_info.min\n",
        "  perp = 1/perp\n",
        "  perp = perp**(1/length)\n",
        "  return perp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZYVc7hB84LP"
      },
      "source": [
        "## Part 4\n",
        "Implement the *match* function that calls *eval* using a specific value of *n* for every possible language pair among the languages we have data for. You should call *eval* for every language pair four times, with each call assign a different value for *n* (1-4). Each language pair is composed of the source language and the target language. Before you make the call, you need to call the *lm* function to create the language model for the source language. Then you can call *eval* with the language model and the target language. The function should return a pandas DataFrame with the following four columns: *source_lang*, *target_lang*, *n*, *perplexity*. The values for the first two columns are the two-letter language codes. The value for *n* is the *n* you use for generating the specific perplexity values which you should store in the forth column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "16ew9aZWroPC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "en 1 en\n",
            "en 1 es\n",
            "en 1 fr\n",
            "en 1 in\n",
            "en 1 it\n",
            "en 1 nl\n",
            "en 1 pt\n",
            "en 1 tl\n",
            "en 2 en\n",
            "en 2 es\n",
            "en 2 fr\n",
            "en 2 in\n",
            "en 2 it\n",
            "en 2 nl\n",
            "en 2 pt\n",
            "en 2 tl\n",
            "en 3 en\n",
            "en 3 es\n",
            "en 3 fr\n",
            "en 3 in\n",
            "en 3 it\n",
            "en 3 nl\n",
            "en 3 pt\n",
            "en 3 tl\n",
            "en 4 en\n",
            "en 4 es\n",
            "en 4 fr\n",
            "en 4 in\n",
            "en 4 it\n",
            "en 4 nl\n",
            "en 4 pt\n",
            "en 4 tl\n",
            "es 1 en\n",
            "es 1 es\n",
            "es 1 fr\n",
            "es 1 in\n",
            "es 1 it\n",
            "es 1 nl\n",
            "es 1 pt\n",
            "es 1 tl\n",
            "es 2 en\n",
            "es 2 es\n",
            "es 2 fr\n",
            "es 2 in\n",
            "es 2 it\n",
            "es 2 nl\n",
            "es 2 pt\n",
            "es 2 tl\n",
            "es 3 en\n",
            "es 3 es\n",
            "es 3 fr\n",
            "es 3 in\n",
            "es 3 it\n",
            "es 3 nl\n",
            "es 3 pt\n",
            "es 3 tl\n",
            "es 4 en\n",
            "es 4 es\n",
            "es 4 fr\n",
            "es 4 in\n",
            "es 4 it\n",
            "es 4 nl\n",
            "es 4 pt\n",
            "es 4 tl\n",
            "fr 1 en\n",
            "fr 1 es\n",
            "fr 1 fr\n",
            "fr 1 in\n",
            "fr 1 it\n",
            "fr 1 nl\n",
            "fr 1 pt\n",
            "fr 1 tl\n",
            "fr 2 en\n",
            "fr 2 es\n",
            "fr 2 fr\n",
            "fr 2 in\n",
            "fr 2 it\n",
            "fr 2 nl\n",
            "fr 2 pt\n",
            "fr 2 tl\n",
            "fr 3 en\n",
            "fr 3 es\n",
            "fr 3 fr\n",
            "fr 3 in\n",
            "fr 3 it\n",
            "fr 3 nl\n",
            "fr 3 pt\n",
            "fr 3 tl\n",
            "fr 4 en\n",
            "fr 4 es\n",
            "fr 4 fr\n",
            "fr 4 in\n",
            "fr 4 it\n",
            "fr 4 nl\n",
            "fr 4 pt\n",
            "fr 4 tl\n",
            "in 1 en\n",
            "in 1 es\n",
            "in 1 fr\n",
            "in 1 in\n",
            "in 1 it\n",
            "in 1 nl\n",
            "in 1 pt\n",
            "in 1 tl\n",
            "in 2 en\n",
            "in 2 es\n",
            "in 2 fr\n",
            "in 2 in\n",
            "in 2 it\n",
            "in 2 nl\n",
            "in 2 pt\n",
            "in 2 tl\n",
            "in 3 en\n",
            "in 3 es\n",
            "in 3 fr\n",
            "in 3 in\n",
            "in 3 it\n",
            "in 3 nl\n",
            "in 3 pt\n",
            "in 3 tl\n",
            "in 4 en\n",
            "in 4 es\n",
            "in 4 fr\n",
            "in 4 in\n",
            "in 4 it\n",
            "in 4 nl\n",
            "in 4 pt\n",
            "in 4 tl\n",
            "it 1 en\n",
            "it 1 es\n",
            "it 1 fr\n",
            "it 1 in\n",
            "it 1 it\n",
            "it 1 nl\n",
            "it 1 pt\n",
            "it 1 tl\n",
            "it 2 en\n",
            "it 2 es\n",
            "it 2 fr\n",
            "it 2 in\n",
            "it 2 it\n",
            "it 2 nl\n",
            "it 2 pt\n",
            "it 2 tl\n",
            "it 3 en\n",
            "it 3 es\n",
            "it 3 fr\n",
            "it 3 in\n",
            "it 3 it\n",
            "it 3 nl\n",
            "it 3 pt\n",
            "it 3 tl\n",
            "it 4 en\n",
            "it 4 es\n",
            "it 4 fr\n",
            "it 4 in\n",
            "it 4 it\n",
            "it 4 nl\n",
            "it 4 pt\n",
            "it 4 tl\n",
            "nl 1 en\n",
            "nl 1 es\n",
            "nl 1 fr\n",
            "nl 1 in\n",
            "nl 1 it\n",
            "nl 1 nl\n",
            "nl 1 pt\n",
            "nl 1 tl\n",
            "nl 2 en\n",
            "nl 2 es\n",
            "nl 2 fr\n",
            "nl 2 in\n",
            "nl 2 it\n",
            "nl 2 nl\n",
            "nl 2 pt\n",
            "nl 2 tl\n",
            "nl 3 en\n",
            "nl 3 es\n",
            "nl 3 fr\n",
            "nl 3 in\n",
            "nl 3 it\n",
            "nl 3 nl\n",
            "nl 3 pt\n",
            "nl 3 tl\n",
            "nl 4 en\n",
            "nl 4 es\n",
            "nl 4 fr\n",
            "nl 4 in\n",
            "nl 4 it\n",
            "nl 4 nl\n",
            "nl 4 pt\n",
            "nl 4 tl\n",
            "pt 1 en\n",
            "pt 1 es\n",
            "pt 1 fr\n",
            "pt 1 in\n",
            "pt 1 it\n",
            "pt 1 nl\n",
            "pt 1 pt\n",
            "pt 1 tl\n",
            "pt 2 en\n",
            "pt 2 es\n",
            "pt 2 fr\n",
            "pt 2 in\n",
            "pt 2 it\n",
            "pt 2 nl\n",
            "pt 2 pt\n",
            "pt 2 tl\n",
            "pt 3 en\n",
            "pt 3 es\n",
            "pt 3 fr\n",
            "pt 3 in\n",
            "pt 3 it\n",
            "pt 3 nl\n",
            "pt 3 pt\n",
            "pt 3 tl\n",
            "pt 4 en\n",
            "pt 4 es\n",
            "pt 4 fr\n",
            "pt 4 in\n",
            "pt 4 it\n",
            "pt 4 nl\n",
            "pt 4 pt\n",
            "pt 4 tl\n",
            "tl 1 en\n",
            "tl 1 es\n",
            "tl 1 fr\n",
            "tl 1 in\n",
            "tl 1 it\n",
            "tl 1 nl\n",
            "tl 1 pt\n",
            "tl 1 tl\n",
            "tl 2 en\n",
            "tl 2 es\n",
            "tl 2 fr\n",
            "tl 2 in\n",
            "tl 2 it\n",
            "tl 2 nl\n",
            "tl 2 pt\n",
            "tl 2 tl\n",
            "tl 3 en\n",
            "tl 3 es\n",
            "tl 3 fr\n",
            "tl 3 in\n",
            "tl 3 it\n",
            "tl 3 nl\n",
            "tl 3 pt\n",
            "tl 3 tl\n",
            "tl 4 en\n",
            "tl 4 es\n",
            "tl 4 fr\n",
            "tl 4 in\n",
            "tl 4 it\n",
            "tl 4 nl\n",
            "tl 4 pt\n",
            "tl 4 tl\n"
          ]
        }
      ],
      "source": [
        "def match() -> pd.DataFrame:\n",
        "  '''\n",
        "  Return a DataFrame containing one line per every language pair and n_gram.\n",
        "  Each line will contain the perplexity calculated when applying the language model\n",
        "  of the source language on the text of the target language.\n",
        "  :return: a DataFrame containing the perplexity values\n",
        "  '''\n",
        "\n",
        "  languages = ['en', 'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "  n_grams = [1, 2, 3, 4]\n",
        "  # languages = languages[:2]\n",
        "  # n_grams = n_grams[:2]\n",
        "\n",
        "  data = []\n",
        "  for source_lang in languages:\n",
        "    for n in n_grams:\n",
        "      model = lm(source_lang, n)\n",
        "      for target_lang in languages:\n",
        "        print(source_lang, n, target_lang)\n",
        "        data.append([source_lang, target_lang, n, eval(model, target_lang)])\n",
        "\n",
        "  return pd.DataFrame(data, columns=['source', 'target', 'n', 'perplexity'])\n",
        "\n",
        "match1 = match()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    source target  n  perplexity\n",
            "144     it     en  3         inf\n",
            "84      fr     it  3         inf\n",
            "87      fr     tl  3         inf\n",
            "112     in     en  3         inf\n",
            "116     in     it  3         inf\n",
            "118     in     pt  3         inf\n",
            "119     in     tl  3         inf\n",
            "149     it     nl  3         inf\n",
            "151     it     tl  3         inf\n",
            "176     nl     en  3         inf\n",
            "179     nl     in  3         inf\n",
            "180     nl     it  3         inf\n",
            "182     nl     pt  3         inf\n",
            "183     nl     tl  3         inf\n",
            "208     pt     en  3         inf\n",
            "211     pt     in  3         inf\n",
            "213     pt     nl  3         inf\n",
            "243     tl     in  3         inf\n",
            "215     pt     tl  3         inf\n",
            "85      fr     nl  3         inf\n",
            "117     in     nl  3         inf\n",
            "83      fr     in  3         inf\n",
            "54      es     pt  3         inf\n",
            "20      en     it  3         inf\n",
            "21      en     nl  3         inf\n",
            "23      en     tl  3         inf\n",
            "48      es     en  3         inf\n",
            "246     tl     pt  3         inf\n",
            "51      es     in  3         inf\n",
            "53      es     nl  3         inf\n",
            "52      es     it  3         inf\n",
            "55      es     tl  3         inf\n",
            "245     tl     nl  3         inf\n",
            "80      fr     en  3         inf\n",
            "147     it     in  3   34.007907\n",
            "19      en     in  3   30.196320\n",
            "114     in     fr  3   29.329540\n",
            "242     tl     fr  3   29.112523\n",
            "210     pt     fr  3   27.323440\n",
            "22      en     pt  3   26.987107\n",
            "177     nl     es  3   25.645368\n",
            "18      en     fr  3   25.245765\n",
            "50      es     fr  3   25.125707\n",
            "178     nl     fr  3   24.947817\n",
            "146     it     fr  3   24.913333\n",
            "113     in     es  3   24.803451\n",
            "86      fr     pt  3   24.758395\n",
            "17      en     es  3   23.554081\n",
            "244     tl     it  3   22.604450\n",
            "241     tl     es  3   22.492380\n",
            "150     it     pt  3   21.729792\n",
            "212     pt     it  3   21.507389\n",
            "81      fr     es  3   21.261716\n",
            "145     it     es  3   19.651128\n",
            "240     tl     en  3   18.862736\n",
            "209     pt     es  3   18.198928\n",
            "115     in     in  3    9.861825\n",
            "181     nl     nl  3    9.151057\n",
            "16      en     en  3    8.770077\n",
            "247     tl     tl  3    8.708958\n",
            "82      fr     fr  3    8.491578\n",
            "148     it     it  3    8.411911\n",
            "49      es     es  3    8.382985\n",
            "214     pt     pt  3    7.864188\n"
          ]
        }
      ],
      "source": [
        "# match1.to_csv('match1.csv', index=False)\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
        "    print(match1.loc[match1['n'] == 3].sort_values(by='perplexity', ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAQoR0dH9C3T"
      },
      "source": [
        "## Part 5\n",
        "Implement the *generate* function which takes a language code, *n*, the prompt (the starting text), the number of tokens to generate, and *r*, which is the random seed for any randomized action you plan to take in your implementation. The function should start generating tokens, one by one, using the language model of the given source language and *n*. The prompt should be used as a starting point for aligning on the probabilities to be used for generating the next token.\n",
        "\n",
        "Note - The generation of the next token should be from the LM's distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CpCm24-RrpuA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "how are you friend?\n",
            "how are you friend? \n",
            "how are you friend? h\n",
            "how are you friend? ht\n",
            "how are you friend? htt\n",
            "how are you friend? http\n",
            "how are you friend? https\n",
            "how are you friend? https:\n",
            "how are you friend? https:/\n",
            "how are you friend? https://\n",
            "how are you friend? https://t\n",
            "how are you friend? https://t.\n",
            "how are you friend? https://t.c\n",
            "how are you friend? https://t.co\n",
            "how are you friend? https://t.co/\n",
            "how are you friend? https://t.co/K\n",
            "how are you friend? https://t.co/K1\n",
            "how are you friend? https://t.co/K19\n",
            "how are you friend? https://t.co/K19K\n",
            "how are you friend? https://t.co/K19Kp\n",
            "how are you friend? https://t.co/K19Kp1\n",
            "how are you friend? https://t.co/K19Kp1d\n",
            "how are you friend? https://t.co/K19Kp1dj\n",
            "how are you friend? https://t.co/K19Kp1djs\n",
            "how are you friend? https://t.co/K19Kp1djsT\n",
            "how are you friend? https://t.co/K19Kp1djsT \n",
            "how are you friend? https://t.co/K19Kp1djsT h\n",
            "how are you friend? https://t.co/K19Kp1djsT ht\n",
            "how are you friend? https://t.co/K19Kp1djsT htt\n",
            "how are you friend? https://t.co/K19Kp1djsT http\n",
            "how are you friend? https://t.co/K19Kp1djsT https\n",
            "how are you friend? https://t.co/K19Kp1djsT https:\n",
            "how are you friend? https://t.co/K19Kp1djsT https:/\n",
            "how are you friend? https://t.co/K19Kp1djsT https://\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.c\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/P\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PB\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBb\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbB\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBD\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDd\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdN\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#f\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#fl\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#fla\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flak\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake i\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it w\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wa\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wan\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wann\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wanna\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaD\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDa\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDai\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDail\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDaily\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyM\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe:\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I k\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I ke\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I kee\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep f\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@S\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@St\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Sto\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor f\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor fe\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor fem\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor fema\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femal\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor female\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleT\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTe\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTex\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleText\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts:\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #B\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Bl\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Bla\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Blac\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black a\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black an\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and r\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and ra\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rat\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rati\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and ratin\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating s\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating so\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sou\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating soun\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound s\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound st\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound sto\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop:\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@j\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@jo\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joe\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joey\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyB\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBA\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBAD\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADA\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADAS\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS'\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"L\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Lo\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Lov\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Love\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves h\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves ht\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves htt\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves http\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https:\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https:/\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.c\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/W\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/Wm\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/Wmz\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/Wmzb\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbB\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBk\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkx\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxe\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0D\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0De\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Des\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Desc\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Descr\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Descri\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describ\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe:\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: \n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: h\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: ht\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: htt\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: http\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: https\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: https:\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: https:/\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: https://\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: https://t\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: https://t.\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: https://t.c\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: https://t.co\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: https://t.co/\n",
            "how are you friend? https://t.co/K19Kp1djsT https://t.co/PBbBDdNM9G#flake it wannaDailyMe: I keep fi…@Stor femaleTexts: #Black and rating sound stop: .@joeyBADASS's \"Loves https://t.co/WmzbBkxeL0Describe: https://t.co/W\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "219"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def generate(lang: str, n: int, prompt: str, number_of_tokens: int, r: int) -> str:\n",
        "  '''\n",
        "  Generate text in the given language using the given parameters.\n",
        "  :param lang: the language of the model\n",
        "  :param n: the n_gram value\n",
        "  :param prompt: the prompt to start the generation\n",
        "  :param number_of_tokens: the number of tokens to generate\n",
        "  :param r: the random seed to use\n",
        "  '''\n",
        "  model = lm(lang, n)\n",
        "  length = len(prompt)\n",
        "  print(prompt)\n",
        "  for i in range(number_of_tokens):\n",
        "    # print(i)\n",
        "    ngram = prompt[-n+1:]\n",
        "    # print(ngram)\n",
        "    prompt += random_from_distribution_dict(model[ngram])\n",
        "    print(prompt)\n",
        "\n",
        "  return prompt\n",
        "\n",
        "def random_from_distribution_dict(dct):\n",
        "    rand_val = np.random.rand()\n",
        "    total = 0\n",
        "    for k, v in dct.items():\n",
        "        total += v\n",
        "        if rand_val <= total:\n",
        "            return k\n",
        "    assert False, 'unreachable'\n",
        "\n",
        "\n",
        "\n",
        "prompt = 'how are you friend?'\n",
        "len(generate('en', 5, prompt, 200, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "match1.shape\n",
        "df=match1\n",
        "df[(df['source'] == 'en') & (df['target'] == 'en') & (df['n'] == 1)]['perplexity'].values[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUWX8Ugu9INH"
      },
      "source": [
        "## Part 6\n",
        "Play with your generate function, try to generate different texts in different language and various values of *n*. No need to submit anything of that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2jNlDISr9aL"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv48OCT_sIYW"
      },
      "source": [
        "Copy the content of the **tests.py** file from the repo and paste below. This will create the results.json file and download it to your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "JZTlc2ieruqq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running test: test_preprocess\n",
            "Running test: test_lm\n",
            "Running test: test_eval\n",
            "Running test: test_match\n",
            "en 1 en\n",
            "en 1 es\n",
            "en 1 fr\n",
            "en 1 in\n",
            "en 1 it\n",
            "en 1 nl\n",
            "en 1 pt\n",
            "en 1 tl\n",
            "en 2 en\n",
            "en 2 es\n",
            "en 2 fr\n",
            "en 2 in\n",
            "en 2 it\n",
            "en 2 nl\n",
            "en 2 pt\n",
            "en 2 tl\n",
            "en 3 en\n",
            "en 3 es\n",
            "en 3 fr\n",
            "en 3 in\n",
            "en 3 it\n",
            "en 3 nl\n",
            "en 3 pt\n",
            "en 3 tl\n",
            "en 4 en\n",
            "en 4 es\n",
            "en 4 fr\n",
            "en 4 in\n",
            "en 4 it\n",
            "en 4 nl\n",
            "en 4 pt\n",
            "en 4 tl\n",
            "es 1 en\n",
            "es 1 es\n",
            "es 1 fr\n",
            "es 1 in\n",
            "es 1 it\n",
            "es 1 nl\n",
            "es 1 pt\n",
            "es 1 tl\n",
            "es 2 en\n",
            "es 2 es\n",
            "es 2 fr\n",
            "es 2 in\n",
            "es 2 it\n",
            "es 2 nl\n",
            "es 2 pt\n",
            "es 2 tl\n",
            "es 3 en\n",
            "es 3 es\n",
            "es 3 fr\n",
            "es 3 in\n",
            "es 3 it\n",
            "es 3 nl\n",
            "es 3 pt\n",
            "es 3 tl\n",
            "es 4 en\n",
            "es 4 es\n",
            "es 4 fr\n",
            "es 4 in\n",
            "es 4 it\n",
            "es 4 nl\n",
            "es 4 pt\n",
            "es 4 tl\n",
            "fr 1 en\n",
            "fr 1 es\n",
            "fr 1 fr\n",
            "fr 1 in\n",
            "fr 1 it\n",
            "fr 1 nl\n",
            "fr 1 pt\n",
            "fr 1 tl\n",
            "fr 2 en\n",
            "fr 2 es\n",
            "fr 2 fr\n",
            "fr 2 in\n",
            "fr 2 it\n",
            "fr 2 nl\n",
            "fr 2 pt\n",
            "fr 2 tl\n",
            "fr 3 en\n",
            "fr 3 es\n",
            "fr 3 fr\n",
            "fr 3 in\n",
            "fr 3 it\n",
            "fr 3 nl\n",
            "fr 3 pt\n",
            "fr 3 tl\n",
            "fr 4 en\n",
            "fr 4 es\n",
            "fr 4 fr\n",
            "fr 4 in\n",
            "fr 4 it\n",
            "fr 4 nl\n",
            "fr 4 pt\n",
            "fr 4 tl\n",
            "in 1 en\n",
            "in 1 es\n",
            "in 1 fr\n",
            "in 1 in\n",
            "in 1 it\n",
            "in 1 nl\n",
            "in 1 pt\n",
            "in 1 tl\n",
            "in 2 en\n",
            "in 2 es\n",
            "in 2 fr\n",
            "in 2 in\n",
            "in 2 it\n",
            "in 2 nl\n",
            "in 2 pt\n",
            "in 2 tl\n",
            "in 3 en\n",
            "in 3 es\n",
            "in 3 fr\n",
            "in 3 in\n",
            "in 3 it\n",
            "in 3 nl\n",
            "in 3 pt\n",
            "in 3 tl\n",
            "in 4 en\n",
            "in 4 es\n",
            "in 4 fr\n",
            "in 4 in\n",
            "in 4 it\n",
            "in 4 nl\n",
            "in 4 pt\n",
            "in 4 tl\n",
            "it 1 en\n",
            "it 1 es\n",
            "it 1 fr\n",
            "it 1 in\n",
            "it 1 it\n",
            "it 1 nl\n",
            "it 1 pt\n",
            "it 1 tl\n",
            "it 2 en\n",
            "it 2 es\n",
            "it 2 fr\n",
            "it 2 in\n",
            "it 2 it\n",
            "it 2 nl\n",
            "it 2 pt\n",
            "it 2 tl\n",
            "it 3 en\n",
            "it 3 es\n",
            "it 3 fr\n",
            "it 3 in\n",
            "it 3 it\n",
            "it 3 nl\n",
            "it 3 pt\n",
            "it 3 tl\n",
            "it 4 en\n",
            "it 4 es\n",
            "it 4 fr\n",
            "it 4 in\n",
            "it 4 it\n",
            "it 4 nl\n",
            "it 4 pt\n",
            "it 4 tl\n",
            "nl 1 en\n",
            "nl 1 es\n",
            "nl 1 fr\n",
            "nl 1 in\n",
            "nl 1 it\n",
            "nl 1 nl\n",
            "nl 1 pt\n",
            "nl 1 tl\n",
            "nl 2 en\n",
            "nl 2 es\n",
            "nl 2 fr\n",
            "nl 2 in\n",
            "nl 2 it\n",
            "nl 2 nl\n",
            "nl 2 pt\n",
            "nl 2 tl\n",
            "nl 3 en\n",
            "nl 3 es\n",
            "nl 3 fr\n",
            "nl 3 in\n",
            "nl 3 it\n",
            "nl 3 nl\n",
            "nl 3 pt\n",
            "nl 3 tl\n",
            "nl 4 en\n",
            "nl 4 es\n",
            "nl 4 fr\n",
            "nl 4 in\n",
            "nl 4 it\n",
            "nl 4 nl\n",
            "nl 4 pt\n",
            "nl 4 tl\n",
            "pt 1 en\n",
            "pt 1 es\n",
            "pt 1 fr\n",
            "pt 1 in\n",
            "pt 1 it\n",
            "pt 1 nl\n",
            "pt 1 pt\n",
            "pt 1 tl\n",
            "pt 2 en\n",
            "pt 2 es\n",
            "pt 2 fr\n",
            "pt 2 in\n",
            "pt 2 it\n",
            "pt 2 nl\n",
            "pt 2 pt\n",
            "pt 2 tl\n",
            "pt 3 en\n",
            "pt 3 es\n",
            "pt 3 fr\n",
            "pt 3 in\n",
            "pt 3 it\n",
            "pt 3 nl\n",
            "pt 3 pt\n",
            "pt 3 tl\n",
            "pt 4 en\n",
            "pt 4 es\n",
            "pt 4 fr\n",
            "pt 4 in\n",
            "pt 4 it\n",
            "pt 4 nl\n",
            "pt 4 pt\n",
            "pt 4 tl\n",
            "tl 1 en\n",
            "tl 1 es\n",
            "tl 1 fr\n",
            "tl 1 in\n",
            "tl 1 it\n",
            "tl 1 nl\n",
            "tl 1 pt\n",
            "tl 1 tl\n",
            "tl 2 en\n",
            "tl 2 es\n",
            "tl 2 fr\n",
            "tl 2 in\n",
            "tl 2 it\n",
            "tl 2 nl\n",
            "tl 2 pt\n",
            "tl 2 tl\n",
            "tl 3 en\n",
            "tl 3 es\n",
            "tl 3 fr\n",
            "tl 3 in\n",
            "tl 3 it\n",
            "tl 3 nl\n",
            "tl 3 pt\n",
            "tl 3 tl\n",
            "tl 4 en\n",
            "tl 4 es\n",
            "tl 4 fr\n",
            "tl 4 in\n",
            "tl 4 it\n",
            "tl 4 nl\n",
            "tl 4 pt\n",
            "tl 4 tl\n",
            "Running test: test_generate\n",
            "I am\n",
            "I ame\n",
            "I amee\n",
            "I amees\n",
            "I ameess\n",
            "I ameess \n",
            "I ameess t\n",
            "I ameess t \n",
            "I ameess t s\n",
            "I ameess t s \n",
            "I ameess t s m\n",
            "I ameess t s mb\n",
            "I ameess t s mbe\n",
            "I ameess t s mbe \n",
            "I ameess t s mbe S\n",
            "I ameess t s mbe St\n",
            "I ameess t s mbe Sto\n",
            "I ameess t s mbe Stor\n",
            "I ameess t s mbe Store\n",
            "I ameess t s mbe Storer\n",
            "I ameess t s mbe Storer \n",
            "I am\n",
            "I ama\n",
            "I amay\n",
            "I amay \n",
            "I amay h\n",
            "I amay ht\n",
            "I amay htt\n",
            "I amay http\n",
            "I amay https\n",
            "I amay https:\n",
            "I amay https:/\n",
            "I amay https://\n",
            "I amay https://t\n",
            "I amay https://t.\n",
            "I amay https://t.c\n",
            "I amay https://t.co\n",
            "I amay https://t.co/\n",
            "I amay https://t.co/A\n",
            "I amay https://t.co/A5\n",
            "I amay https://t.co/A50\n",
            "I amay https://t.co/A50K\n",
            "I Love\n",
            "I Love \n",
            "I Love a\n",
            "I Love ab\n",
            "I Love abd\n",
            "I Love abdl\n",
            "I Love abdld\n",
            "I Love abdldb\n",
            "I Love abdldbe\n",
            "I Love abdldbeg\n",
            "I Love abdldbegi\n",
            "I Love abdldbegia\n",
            "I Love abdldbegiaD\n",
            "I Love abdldbegiaDi\n",
            "I Love abdldbegiaDin\n",
            "I Love abdldbegiaDinn\n",
            "I Love abdldbegiaDinne\n",
            "I Love abdldbegiaDinner\n",
            "I Love abdldbegiaDinner'\n",
            "I Love abdldbegiaDinner's\n",
            "I Love abdldbegiaDinner's \n",
            "Soy\n",
            "Soyn\n",
            "Soynh\n",
            "Soynhe\n",
            "Soynhe \n",
            "Soynhe  \n",
            "Soynhe  j\n",
            "Soynhe  jo\n",
            "Soynhe  jo \n",
            "Soynhe  jo T\n",
            "Soynhe  jo Te\n",
            "Soynhe  jo Tes\n",
            "Soynhe  jo Tess\n",
            "Soynhe  jo Tess \n",
            "Soynhe  jo Tess d\n",
            "Soynhe  jo Tess do\n",
            "Soynhe  jo Tess do/\n",
            "Soynhe  jo Tess do/ \n",
            "Soynhe  jo Tess do/ h\n",
            "Soynhe  jo Tess do/ ha\n",
            "Soynhe  jo Tess do/ had\n",
            "Soy\n",
            "Soy \n",
            "Soy y\n",
            "Soy y \n",
            "Soy y p\n",
            "Soy y pa\n",
            "Soy y pañ\n",
            "Soy y paña\n",
            "Soy y paña \n",
            "Soy y paña n\n",
            "Soy y paña no\n",
            "Soy y paña non\n",
            "Soy y paña noni\n",
            "Soy y paña nonie\n",
            "Soy y paña nonier\n",
            "Soy y paña noniero\n",
            "Soy y paña noniero¡\n",
            "Soy y paña noniero¡¡\n",
            "Soy y paña noniero¡¡¡\n",
            "Soy y paña noniero¡¡¡¡\n",
            "Soy y paña noniero¡¡¡¡“\n",
            "Je suis\n",
            "Je suis \n",
            "Je suis t\n",
            "Je suis t \n",
            "Je suis t m\n",
            "Je suis t ma\n",
            "Je suis t mar\n",
            "Je suis t mare\n",
            "Je suis t mare \n",
            "Je suis t mare l\n",
            "Je suis t mare le\n",
            "Je suis t mare lex\n",
            "Je suis t mare lex \n",
            "Je suis t mare lex  \n",
            "Je suis t mare lex  d\n",
            "Je suis t mare lex  di\n",
            "Je suis t mare lex  die\n",
            "Je suis t mare lex  dien\n",
            "Je suis t mare lex  diens\n",
            "Je suis t mare lex  dienso\n",
            "Je suis t mare lex  diensou\n",
            "Je suis\n",
            "Je suist\n",
            "Je suisti\n",
            "Je suistio\n",
            "Je suistion\n",
            "Je suistion \n",
            "Je suistion m\n",
            "Je suistion ma\n",
            "Je suistion ma \n",
            "Je suistion ma j\n",
            "Je suistion ma jv\n",
            "Je suistion ma jvo\n",
            "Je suistion ma jvou\n",
            "Je suistion ma jvour\n",
            "Je suistion ma jvouro\n",
            "Je suistion ma jvouro:\n",
            "Je suistion ma jvouro: \n",
            "Je suistion ma jvouro: L\n",
            "Je suistion ma jvouro: Le\n",
            "Je suistion ma jvouro: LeV\n",
            "Je suistion ma jvouro: LeVS\n"
          ]
        }
      ],
      "source": [
        "####################\n",
        "# PLACE TESTS HERE #\n",
        "\n",
        "# Create tests\n",
        "def test_preprocess():\n",
        "    return {\n",
        "        'vocab_length': len(preprocess()),\n",
        "    }\n",
        "\n",
        "def test_lm():\n",
        "    return {\n",
        "        'english_2_gram_length': len(lm('en', 2)),\n",
        "        'english_3_gram_length': len(lm('en', 3)),\n",
        "        'french_3_gram_length': len(lm('fr', 3)),\n",
        "        'spanish_3_gram_length': len(lm('es', 3)),\n",
        "    }\n",
        "\n",
        "def test_eval():\n",
        "    return {\n",
        "        'english_on_english': round(eval(lm('en', 3), 'en'), 2),\n",
        "        'english_on_french': round(eval(lm('en', 3), 'fr'), 2),\n",
        "        'english_on_spanish': round(eval(lm('en', 3), 'es'), 2),\n",
        "    }\n",
        "\n",
        "def test_match():\n",
        "    df = match()\n",
        "    return {\n",
        "        'df_shape': df.shape,\n",
        "        'en_en_1': df[(df['source'] == 'en') & (df['target'] == 'en') & (df['n'] == 1)]['perplexity'].values[0],\n",
        "        'tl_tl_1': df[(df['source'] == 'tl') & (df['target'] == 'tl') & (df['n'] == 1)]['perplexity'].values[0],\n",
        "        'tl_nl_4': df[(df['source'] == 'tl') & (df['target'] == 'nl') & (df['n'] == 4)]['perplexity'].values[0],\n",
        "    }\n",
        "\n",
        "def test_generate():\n",
        "    return {\n",
        "        'english_2_gram': generate('en', 2, \"I am\", 20, 5),\n",
        "        'english_3_gram': generate('en', 3, \"I am\", 20, 5),\n",
        "        'english_4_gram': generate('en', 4, \"I Love\", 20, 5),\n",
        "        'spanish_2_gram': generate('es', 2, \"Soy\", 20, 5),\n",
        "        'spanish_3_gram': generate('es', 3, \"Soy\", 20, 5),\n",
        "        'french_2_gram': generate('fr', 2, \"Je suis\", 20, 5),\n",
        "        'french_3_gram': generate('fr', 3, \"Je suis\", 20, 5),\n",
        "    }\n",
        "\n",
        "TESTS = [test_preprocess, test_lm, test_eval, test_match, test_generate]\n",
        "\n",
        "# Run tests and save results\n",
        "res = {}\n",
        "for test in TESTS:\n",
        "    print(f'Running test: {test.__name__}')\n",
        "    try:\n",
        "        cur_res = test()\n",
        "        res.update({test.__name__: cur_res})\n",
        "    except Exception as e:\n",
        "        res.update({test.__name__: repr(e)})\n",
        "\n",
        "\n",
        "# save the result file as results.json\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(res, f, indent=2)\n",
        "\n",
        "\n",
        "# with open('results.json', 'w') as f:\n",
        "#     json.dump(res, f, indent=2)\n",
        "\n",
        "# Download the results.json file\n",
        "# files.download('results.json')\n",
        "\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCksAA6RisRQ",
        "outputId": "ca69cf19-9e4c-446e-b0c6-3f87b02f1d8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# Show the local files, results.json should be there now and\n",
        "# also downloaded to your local machine\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMSfgUtuiux0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
